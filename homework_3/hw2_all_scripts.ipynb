{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Supercomputer.py file #\n",
    "#########################\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from collections import deque\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from models import dnn\n",
    "from symbiotic_metrics import FractionOfVarianceAccountedFor\n",
    "\n",
    "def main():\n",
    "\n",
    "    rotation_list = list(range(20))\n",
    "    n_train_folds_list = [1, 2, 3, 5, 10, 18]\n",
    "\n",
    "    create_index_log(rotation_list, n_train_folds_list)\n",
    "\n",
    "    for rotation in rotation_list:\n",
    "        for n_train_folds in n_train_folds_list:\n",
    "            start_training_job(rotation, n_train_folds)\n",
    "\n",
    "def start_training_job(rotation, n_train_folds):\n",
    "\n",
    "    print(\"Starting job: Rotation {:02d}, # Training Folds {:02d}\".format(rotation, n_train_folds))\n",
    "\n",
    "    if \"-s\" in sys.argv:\n",
    "        script_to_run = [\"sbatch\", \"supercomputer_job.sh\", \"-s\"]\n",
    "    else:\n",
    "        script_to_run = [\"./standard_job.sh\"]\n",
    "\n",
    "    process = subprocess.Popen(\n",
    "        [*script_to_run, \n",
    "        \"-job\", # Indicate to the subprocess that it is a subprocess\n",
    "        \"-rotation={}\".format(rotation),\n",
    "        \"-n_train_folds={}\".format(n_train_folds)\n",
    "        ])\n",
    "\n",
    "    if \"-p\" not in sys.argv:\n",
    "        process.wait()\n",
    "\n",
    "def parse_args():\n",
    "\n",
    "    for arg in sys.argv:\n",
    "        if \"-rotation=\" in arg:\n",
    "            rotation = int(arg.replace(\"-rotation=\", \"\"))\n",
    "        elif \"-n_train_folds=\" in arg:\n",
    "            n_train_folds = int(arg.replace(\"-n_train_folds=\", \"\"))\n",
    "\n",
    "    return rotation, n_train_folds\n",
    "\n",
    "def train(rotation=0, n_train_folds=18):\n",
    "\n",
    "    print(\"PARAMETERS: Rotation {:02d}, # Training Folds {:02d}\".format(rotation, n_train_folds))\n",
    "    \n",
    "    # Rotate indices based on current rotation\n",
    "    rotation_indices = get_rotation_indices(n_folds=20, rotation=rotation)\n",
    "\n",
    "    # Get the training, validation, and test fold indices\n",
    "    fold_inds = get_set_indices(rotation_indices=rotation_indices, n_train_folds=n_train_folds)\n",
    "\n",
    "    ''' Load data\n",
    "    Key MI, Length 20, Shape (1193, 960)\n",
    "    Key theta, Length 20, Shape (1193, 2)\n",
    "    Key dtheta, Length 20, Shape (1193, 2)\n",
    "    Key ddtheta, Length 20, Shape (1193, 2)\n",
    "    Key torque, Length 20, Shape (1193, 2)\n",
    "    Key time, Length 20, Shape (1193, 1)\n",
    "    '''\n",
    "    if \"-s\" in sys.argv:\n",
    "        data_path = \"/home/fagg/ml_datasets/bmi/bmi_dataset.pkl\"\n",
    "    else:\n",
    "        data_path = \"bmi_dataset.pkl\"\n",
    "    with open(data_path, \"rb\") as fp:\n",
    "        hw2_dataset = pickle.load(fp)\n",
    "\n",
    "    # Splits the data into its respective train, validation, and test sets / ins and outs\n",
    "    processed_data = process_dataset(hw2_dataset, fold_inds)\n",
    "\n",
    "    # Build model\n",
    "    model = dnn(\n",
    "        input_size=(processed_data[\"train\"][\"ins\"].shape[1],),\n",
    "        hidden_sizes=[100, 50],\n",
    "        output_size=processed_data[\"train\"][\"outs\"].shape[1],\n",
    "        hidden_act=\"elu\",\n",
    "        output_act=\"linear\")\n",
    "\n",
    "    # Compile model with fvaf metric\n",
    "    fvaf = FractionOfVarianceAccountedFor(processed_data[\"test\"][\"outs\"].shape[1])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[fvaf], verbose=2)\n",
    "    model.summary()\n",
    "\n",
    "    # Callbacks\n",
    "    es_callback = EarlyStopping(\n",
    "                            monitor=\"val_loss\",\n",
    "                            patience=5,\n",
    "                            restore_best_weights=True,\n",
    "                            min_delta=.0001)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "            x=processed_data[\"train\"][\"ins\"],\n",
    "            y=processed_data[\"train\"][\"outs\"],\n",
    "            validation_data = (processed_data[\"val\"][\"ins\"], processed_data[\"val\"][\"outs\"]),\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            callbacks=[es_callback]\n",
    "            )\n",
    "\n",
    "    # Log results\n",
    "    log(model, processed_data, fold_inds, rotation, n_train_folds)\n",
    "\n",
    "    # Plot the torque and save figure\n",
    "    plot_torque(model, processed_data, rotation, n_train_folds)\n",
    "\n",
    "def plot_torque(model, data, rotation, n_train_folds):\n",
    "\n",
    "    save_path = \"results/\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    save_path += \"r{:02d}_t{:02d}/\".format(rotation, n_train_folds)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    true_torque = data[\"test\"][\"outs\"][:, 0]\n",
    "    predicted_torque = model.predict(data[\"test\"][\"ins\"])[:, 0]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(data[\"test\"][\"time\"], true_torque, label=\"True Torque\")\n",
    "    ax.plot(data[\"test\"][\"time\"], predicted_torque, label=\"Predicted Torque\")\n",
    "    ax.legend()\n",
    "    plt.ylabel(\"Torque\")\n",
    "    plt.xlabel(\"Time\")\n",
    "\n",
    "    fig.savefig(save_path + f\"torque_plot.png\", dpi=fig.dpi)\n",
    "\n",
    "def create_index_log(rotation_list, n_train_folds_list):\n",
    "    \n",
    "    index = {\n",
    "        \"rotation_list\": rotation_list,\n",
    "        \"n_train_folds_list\": n_train_folds_list\n",
    "    }\n",
    "\n",
    "    fbase = \"results/\"\n",
    "    if not os.path.exists(fbase):\n",
    "        os.mkdir(fbase)\n",
    "\n",
    "    with open('{}index.json'.format(fbase), 'w') as f:\n",
    "        json.dump(index, f)\n",
    "\n",
    "def log(model, data, fold_inds, rotation, n_train_folds):\n",
    "    print(\"Logging results\")\n",
    "\n",
    "    # Generate results\n",
    "    results = {}\n",
    "    results['predict_train'] = model.predict(data[\"train\"][\"ins\"])\n",
    "    results['eval_train'] = model.evaluate(data[\"train\"][\"ins\"], data[\"train\"][\"outs\"])\n",
    "    results['predict_val'] = model.predict(data[\"val\"][\"ins\"])\n",
    "    results['eval_val'] = model.evaluate(data[\"val\"][\"ins\"], data[\"val\"][\"outs\"])\n",
    "    results['predict_test'] = model.predict(data[\"test\"][\"ins\"])\n",
    "    results['eval_test'] = model.evaluate(data[\"test\"][\"ins\"], data[\"test\"][\"outs\"])\n",
    "    results['folds'] = fold_inds\n",
    "    results['rotation'] = rotation\n",
    "    results['n_train_folds'] = n_train_folds\n",
    "\n",
    "    # Create results directory\n",
    "    fbase = \"results/\"\n",
    "    if not os.path.exists(fbase):\n",
    "        os.mkdir(fbase)\n",
    "    fbase += \"r{:02d}_t{:02d}/\".format(rotation, n_train_folds)\n",
    "    if not os.path.exists(fbase):\n",
    "        os.mkdir(fbase)\n",
    "\n",
    "    # Save results\n",
    "    with open(\"{}results.pkl\".format(fbase, rotation, n_train_folds), \"wb\") as fp:\n",
    "        pickle.dump(results, fp)\n",
    "        fp.close()\n",
    "\n",
    "    # Create model directory\n",
    "    if not os.path.exists(\"{}/model/\".format(fbase)):\n",
    "        os.mkdir(\"{}/model/\".format(fbase))\n",
    "\n",
    "    # Save model\n",
    "    model.save(\"{}/model/\".format(fbase))\n",
    "\n",
    "def process_dataset(dataset, fold_inds):\n",
    "\n",
    "    processed_data = {}\n",
    "    for key in fold_inds.keys():\n",
    "        processed_data[key] = split_dataset(dataset, fold_inds[key])\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def split_dataset(dataset, inds):\n",
    "    \n",
    "    processed_data = {\n",
    "        \"ins\": None,\n",
    "        \"outs\": [],\n",
    "        \"time\": None\n",
    "    }\n",
    "    \n",
    "    for key in dataset.keys():\n",
    "        # Get folds for this key\n",
    "        folds = [dataset[key][ind] for ind in inds]\n",
    "\n",
    "        # Join the folds\n",
    "        joined = np.concatenate((folds), axis=0)\n",
    "\n",
    "        # See if the key is for the ins or outs of the dataset\n",
    "        if key == \"MI\":\n",
    "            processed_data[\"ins\"] = joined\n",
    "        elif key == \"time\":\n",
    "            processed_data[\"time\"] = joined\n",
    "        elif key == \"torque\":\n",
    "            processed_data[\"outs\"] = np.expand_dims(joined[:, 1], axis=1)\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def get_set_indices(rotation_indices, n_train_folds):\n",
    "\n",
    "    inds = {}\n",
    "    inds[\"train\"] = [rotation_indices[i] for i in range(n_train_folds)]\n",
    "    inds[\"val\"] = [rotation_indices[len(rotation_indices)-2]]\n",
    "    inds[\"test\"] = [rotation_indices[len(rotation_indices)-1]]\n",
    "\n",
    "    return inds\n",
    "\n",
    "def get_rotation_indices(n_folds, rotation=0):\n",
    "\n",
    "    fold_list = list(range(n_folds))\n",
    "    fold_list = deque(fold_list)\n",
    "    fold_list.rotate(rotation)\n",
    "    fold_list = list(fold_list)\n",
    "\n",
    "    return fold_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # If this is a subprocess, run the training program\n",
    "    if \"-job\" in sys.argv:\n",
    "        rotation, n_train_folds = parse_args()\n",
    "\n",
    "        try:\n",
    "            train(rotation=rotation, n_train_folds=n_train_folds)\n",
    "        \n",
    "        # If any exception occurs, write to error folder to differentiate between all the job outputs\n",
    "        except Exception as e:\n",
    "            fbase = \"error/\"\n",
    "            if not os.path.exists(fbase):\n",
    "                os.mkdir(fbase)\n",
    "\n",
    "            with open(\"{}r{:02d}_t{:02d}_err.txt\".format(fbase, rotation, n_train_folds), \"a\") as f:\n",
    "                err_str = \"Error: {}\".format(e)\n",
    "                f.write(err_str)\n",
    "\n",
    "    else:\n",
    "        main()\n",
    "\n",
    "#################\n",
    "# local.py file #\n",
    "#################\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Use -s argument to scp results from supercomputer before continuing\n",
    "    if \"-s\" in sys.argv:\n",
    "        script_to_run = [\n",
    "            \"scp\",\n",
    "            \"-r\", \n",
    "            \"jwspaeth@schooner.oscer.ou.edu:/home/jwspaeth/workspaces/advanced-ml/homework_2/results\",\n",
    "            \"./\"]\n",
    "        process = subprocess.Popen([*script_to_run])\n",
    "        process.wait()\n",
    "\n",
    "    # Read index file\n",
    "    index = load_index_log()\n",
    "    rotation_list = index[\"rotation_list\"]\n",
    "    n_train_folds_list = index[\"n_train_folds_list\"]\n",
    "\n",
    "    # Load all results\n",
    "    results = []\n",
    "    for rotation in rotation_list:\n",
    "        for n_train_folds in n_train_folds_list:\n",
    "            with open(\"results/r{:02d}_t{:02d}/results.pkl\".format(rotation, n_train_folds), \"rb\") as fp:\n",
    "                results.append(pickle.load(fp))\n",
    "\n",
    "    # Compute average fvafs\n",
    "    avg_fvafs = compute_avg_fvafs(results, n_train_folds_list)\n",
    "\n",
    "    # Plot and save all the fvafs\n",
    "    plot_fvaf(avg_fvafs, n_train_folds_list, \"train\")\n",
    "    plot_fvaf(avg_fvafs, n_train_folds_list, \"val\")\n",
    "    plot_fvaf(avg_fvafs, n_train_folds_list, \"test\")\n",
    "\n",
    "def load_index_log():\n",
    "\n",
    "    with open(\"results/index.json\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def plot_fvaf(avg_fvafs, n_train_folds_list, set_name):\n",
    "    \n",
    "    save_path = \"results/\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    save_path += \"fvaf_plots/\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(n_train_folds_list, avg_fvafs[set_name])\n",
    "    plt.ylabel(\"Average FVAF\")\n",
    "    plt.xlabel(\"Number of Training Folds\")\n",
    "\n",
    "    if set_name == \"train\":\n",
    "        plt.title(\"Training Set\")\n",
    "    elif set_name == \"val\":\n",
    "        plt.title(\"Validation Set\")\n",
    "    elif set_name == \"test\":\n",
    "        plt.title(\"Test Set\")\n",
    "\n",
    "    fig.savefig(\"{}{}_fvaf_plot.png\".format(save_path, set_name), dpi=fig.dpi)\n",
    "\n",
    "def compute_avg_fvafs(results, n_train_folds_list):\n",
    "    \n",
    "    # Sum all the fvafs and count how many values there are\n",
    "    # Each index represents a n_train_folds hyperparameter\n",
    "    avg_fvafs = {\n",
    "        \"train\": [0]*len(n_train_folds_list),\n",
    "        \"val\": [0]*len(n_train_folds_list),\n",
    "        \"test\": [0]*len(n_train_folds_list)\n",
    "    }\n",
    "\n",
    "    # Loop through each split\n",
    "    for key in avg_fvafs.keys():\n",
    "\n",
    "        # Start summing and count the fvaf values\n",
    "        sum_fvafs = [0]*len(n_train_folds_list)\n",
    "        count_fvafs = [0]*len(n_train_folds_list)\n",
    "        for i in range(len(n_train_folds_list)):\n",
    "\n",
    "            for result in results:\n",
    "                if result[\"n_train_folds\"] == n_train_folds_list[i]:\n",
    "                    sum_fvafs[i] += result[\"eval_{}\".format(key)][1]\n",
    "                    count_fvafs[i] += 1\n",
    "\n",
    "        # Create average fvafs based on the sum and counts\n",
    "        for i in range(len(n_train_folds_list)):\n",
    "            if count_fvafs[i] != 0:\n",
    "                avg_fvafs[key][i] = sum_fvafs[i] / count_fvafs[i]\n",
    "\n",
    "    return avg_fvafs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "##################\n",
    "# models.py file #\n",
    "##################\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "def dnn(input_size, hidden_sizes, output_size, hidden_act=\"sigmoid\", output_act=\"tanh\"):\n",
    "    \"\"\"Construct a simple deep neural network\"\"\"\n",
    "\n",
    "    inputs = Input(shape=input_size)\n",
    "\n",
    "    hidden_stack_out = hidden_stack(hidden_sizes, hidden_act)(inputs)\n",
    "\n",
    "    outputs = Dense(output_size, activation=output_act)(hidden_stack_out)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def hidden_stack(hidden_sizes, hidden_act=\"sigmoid\"):\n",
    "    \"\"\"Represents a stack of neural layers\"\"\"\n",
    "\n",
    "    layers = []\n",
    "    for size in hidden_sizes:\n",
    "        layers.append(Dense(size, activation=hidden_act))\n",
    "\n",
    "    def hidden_stack_layer(inputs):\n",
    "        \"\"\"Layer hook for stack\"\"\"\n",
    "\n",
    "        for i in range(len(layers)):\n",
    "            if i == 0:\n",
    "                carry_out = layers[i](inputs)\n",
    "            else:\n",
    "                carry_out = layers[i](carry_out)\n",
    "\n",
    "        return carry_out\n",
    "\n",
    "    return hidden_stack_layer\n",
    "\n",
    "#############################\n",
    "# supercomputer_job.sh file #\n",
    "#############################\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=normal\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --mem=2000\n",
    "#SBATCH --output=job-output/subprocess-%j-stdout.txt\n",
    "#SBATCH --error=job-output/subprocess--%j-stderr.txt\n",
    "#SBATCH --time=7:00:00\n",
    "#SBATCH --job-name=subprocess-%j\n",
    "#SBATCH --mail-user=john.w.spaeth-1@ou.edu\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --chdir=/home/jwspaeth/workspaces/advanced-ml/homework_2/\n",
    "#SBATCH --wait\n",
    "\n",
    "python3 supercomputer.py $@\n",
    "\n",
    "#################\n",
    "# Torque Figure #\n",
    "#################\n",
    "\n",
    "##############\n",
    "# FVAF Plots #\n",
    "##############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
